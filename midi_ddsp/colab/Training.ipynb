{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036d2de",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall ddsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d3fa2-7dc1-4b91-9d81-2f3de4adc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2e2c8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k, v in d.items():\n",
    "        print(f\"\\t'{k}': {v}\")\n",
    "\n",
    "def print_dict2(d):\n",
    "    l = 0\n",
    "    for detail in d:\n",
    "        print()\n",
    "        print(f'item #{l}:')\n",
    "        l = l + 1\n",
    "\n",
    "        i = 0\n",
    "        for k, v in detail.items():\n",
    "            if i == 0:\n",
    "                print(f\"\\t'{k}': {v}\")\n",
    "            else:\n",
    "                print(f\"\\t\\t'{k}': {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7f612-58b5-4fce-8844-13be9607c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    midi_ddsp_module_path = os.path.abspath(os.path.join('../../'))\n",
    "    ddsp_module_path = os.path.abspath(os.path.join('../../../ddsp-playground-2/'))\n",
    "else:\n",
    "    midi_ddsp_module_path = os.path.abspath(os.path.join('../../'))\n",
    "    ddsp_module_path = os.path.abspath(os.path.join('../../../ddsp/ddsp-playground-2/'))\n",
    "\n",
    "def apply_module_path(module_path):\n",
    "    print(f\"module_path={module_path}\")\n",
    "    if module_path not in sys.path:\n",
    "      sys.path.append(module_path)\n",
    "      print(f\"appending {module_path} to sys.path\")\n",
    "    else:\n",
    "      print(f\"do not appending {module_path} to sys.path\")\n",
    "\n",
    "apply_module_path(midi_ddsp_module_path)\n",
    "apply_module_path(ddsp_module_path)\n",
    "\n",
    "import sys\n",
    "if platform.system() != 'Windows':\n",
    "    sparsenet_module_path_abs = '/ssd003/home/burakovr/projects/vova/envs/main/lib/python3.8/site-packages/'\n",
    "    #apply_module_path(sparsenet_module_path_abs)\n",
    "\n",
    "    libs_modules_path_abs = '/ssd003/home/burakovr/projects/vova/envs/ddsp/lib/python3.8/site-packages/'\n",
    "    apply_module_path(libs_modules_path_abs)\n",
    "\n",
    "import midi_ddsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10449b-60fd-4f51-a439-a4fbc9a36ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using imports from original Magenta's 'train_synthesis_generator.py'. This is not plagiarism, please pay more attention to the code \n",
    "\n",
    "#  Copyright 2022 The MIDI-DDSP Authors.\n",
    "#  #\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#  #\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#  #\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\n",
    "\"\"\"Training code for Synthesis Generator.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import IPython\n",
    "\n",
    "from keras.utils.layer_utils import print_summary\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from midi_ddsp.data_handling.get_dataset import get_dataset\n",
    "from midi_ddsp.utils.training_utils import print_hparams, set_seed, \\\n",
    "    save_results, str2bool\n",
    "from midi_ddsp.utils.summary_utils import write_tensorboard_audio\n",
    "#                          from midi_ddsp.hparams_synthesis_generator import hparams as hp\n",
    "from midi_ddsp.hparams_synthesis_generator import hparams_debug as hp\n",
    "from midi_ddsp.modules.recon_loss import ReconLossHelper\n",
    "from midi_ddsp.modules.gan_loss import GANLossHelper\n",
    "from midi_ddsp.modules.get_synthesis_generator import get_synthesis_generator, \\\n",
    "    get_fake_data_synthesis_generator\n",
    "from midi_ddsp.modules.discriminator import Discriminator\n",
    "from ddsp.colab.notebook_utils import play, specplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c69b0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7ca4a-3944-422c-b45b-8dad04618290",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlosses = PlotLosses()\n",
    "\n",
    "\n",
    "losses_history = [] # per step\n",
    "eval_losses_history = [] # per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00cdbb-a368-4ddf-b1ba-4321efbbb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tflite_model(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    #print(f\"input_details={[detail['name'] for detail in input_details]}\")\n",
    "    print(f\"\\n\\n Inputs (len={len(input_details)}):\")\n",
    "    #print(input_details)\n",
    "    print_dict2(input_details)\n",
    "    \n",
    "    output_details = interpreter.get_output_details()\n",
    "    print(f\"\\n\\n Outputs (len={len(output_details)}):\")\n",
    "    print_dict2(output_details)\n",
    "    \n",
    "    return interpreter, input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f39e3-8b48-40e1-836b-a47048be971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tflite_inference(interpreter, inputs):\n",
    "    \n",
    "    def set_inputs_for_tflite(interpreter, interpreter_input):\n",
    "    #interpreter_input = eval_sample_batch\n",
    "\n",
    "        for key, value in interpreter_input.items():\n",
    "            print(f\"\")\n",
    "            #print(f\"#{i}: trying to set {key}. It's value is {value}\")\n",
    "\n",
    "            good_detail = None\n",
    "            for detail in input_details:\n",
    "                #print(detail['name'][16:-2])\n",
    "                if detail['name'][16:-2] == key or detail['name'] == key:\n",
    "                    good_detail = detail\n",
    "                    break\n",
    "\n",
    "            if good_detail is None:\n",
    "                print(f\"Unable to find {key}. Skipping\")\n",
    "            else:\n",
    "\n",
    "                print(f\"Our raw input value: {tf.shape(value)}\")\n",
    "                print(f\"trying to set {good_detail['name']} with tensor of shape {tf.shape(value)}\")\n",
    "\n",
    "                interpreter.set_tensor(good_detail['index'], tf.cast(value, good_detail['dtype']))\n",
    "\n",
    "    \n",
    "    def get_outputs_from_tflite(interpreter):\n",
    "        #f0_hz  = interpreter.get_tensor(input_details[1]['index'])\n",
    "        #amps   = interpreter.get_tensor(output_details[11]['index'])\n",
    "        #noises = interpreter.get_tensor(output_details[14]['index'])\n",
    "        #hd     = interpreter.get_tensor(output_details[13]['index'])\n",
    "\n",
    "        f0_hz  = interpreter.get_tensor(input_details[1]['index'])\n",
    "        amps   = interpreter.get_tensor(output_details[0]['index'])\n",
    "        noises = interpreter.get_tensor(output_details[3]['index'])\n",
    "        hd     = interpreter.get_tensor(output_details[2]['index'])\n",
    "        \n",
    "        #amps    = tf.reshape(amps,      [1, 1, 1])\n",
    "        #noises  = tf.reshape(noises,    [1, 1, 60])\n",
    "        #hd      = tf.reshape(hd,        [1, 1, 65])\n",
    "\n",
    "        synth_params = {\n",
    "            'f0_hz': f0_hz,\n",
    "            'amplitudes': amps,\n",
    "            'harmonic_distribution': hd,\n",
    "            'noise_magnitudes': noises\n",
    "        }\n",
    "\n",
    "        return synth_params\n",
    "    \n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    set_inputs_for_tflite(interpreter=interpreter, interpreter_input=inputs)\n",
    "    \n",
    "    interpreter.invoke()\n",
    "    \n",
    "    tflite_synth_params = get_outputs_from_tflite(interpreter)\n",
    "    \n",
    "    return tflite_synth_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad36651-ee82-4687-ac9b-7c5ca4b8a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "\n",
    "def resample_tensor(tensor, input_rate, output_rate):\n",
    "    if len(tensor.shape) == 2:\n",
    "        tensor = tf.expand_dims(tensor, axis=-1)  # Convert to shape [1, 1000, 1]\n",
    "\n",
    "    # Resample the tensor\n",
    "    resampled_tensor = tfio.audio.resample(tensor, input_rate, output_rate)\n",
    "\n",
    "    if len(tensor.shape) == 2:\n",
    "        resampled_tensor = tf.squeeze(resampled_tensor, axis=-1)  # Convert back to shape [1, 300]\n",
    "\n",
    "    return resampled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e7811-d9b1-40c2-ab59-94532ebd22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vst_inputs_for_batch(source_batch, buffer_size_in_samples, n_frames_in_buffer):\n",
    "\n",
    "    def get_vst_inputs_for_buffer(buffer_idx, buffer_size_in_samples, n_frames_in_buffer, source_batch):    \n",
    "\n",
    "        dataset_frame_size = source_batch['audio'].shape[1] // source_batch['f0_hz'].shape[1] # 64\n",
    "        n_dataset_frames_in_batch  = source_batch['audio'].shape[1] // dataset_frame_size\n",
    "        n_dataset_frames_in_buffer = buffer_size_in_samples // dataset_frame_size             # 1000\n",
    "        n_dataset_frames_in_one_user_frame = n_dataset_frames_in_buffer // n_frames_in_buffer # 2 ds_frames in one frame\n",
    "        ds_frame_pos = buffer_idx * n_dataset_frames_in_one_user_frame # on which frame the given buffer starts\n",
    "        n_user_frames_in_batch = source_batch['f0_hz'].shape[1] // n_dataset_frames_in_one_user_frame # 500\n",
    "        \n",
    "        bs = tf.shape(source_batch['audio'])[0]\n",
    "        \n",
    "        #resized_batch = tf.image.resize(source_batch, new_shape, method=tf.image.ResizeMethod.LINEAR)\n",
    "        resized_audio         = source_batch['audio']\n",
    "        resized_loudness_db   = resample_tensor(source_batch['loudness_db'], n_dataset_frames_in_batch, n_user_frames_in_batch)\n",
    "        resized_f0_hz         = resample_tensor(source_batch['f0_hz'],       n_dataset_frames_in_batch, n_user_frames_in_batch)\n",
    "        resized_midi          = resample_tensor(source_batch['midi'],        n_dataset_frames_in_batch, n_user_frames_in_batch)\n",
    "        resized_onsets        = resample_tensor(source_batch['onsets'],      n_dataset_frames_in_batch, n_user_frames_in_batch)\n",
    "        resized_offsets       = resample_tensor(source_batch['offsets'],     n_dataset_frames_in_batch, n_user_frames_in_batch)\n",
    "        resized_instrument_id = source_batch['instrument_id']\n",
    "\n",
    "        \n",
    "        audio         = resized_audio[0][buffer_idx*buffer_size_in_samples:(buffer_idx+1)*buffer_size_in_samples]\n",
    "        loudness_db   = tf.reshape(resized_loudness_db[0][ds_frame_pos:ds_frame_pos+n_dataset_frames_in_buffer], [n_frames_in_buffer])\n",
    "        f0_hz         = tf.reshape(resized_f0_hz[0][ds_frame_pos:ds_frame_pos+n_dataset_frames_in_buffer], [n_frames_in_buffer])\n",
    "        midi          = resized_midi[0][ds_frame_pos:ds_frame_pos+n_dataset_frames_in_buffer]\n",
    "        onsets        = resized_onsets[0][ds_frame_pos:ds_frame_pos+n_dataset_frames_in_buffer]\n",
    "        offsets       = resized_offsets[0][ds_frame_pos:ds_frame_pos+n_dataset_frames_in_buffer]\n",
    "        instrument_id = tf.reshape(resized_instrument_id[0], [1])\n",
    "\n",
    "        return {'audio': audio, 'loudness_db': loudness_db, 'f0_hz': f0_hz, 'midi': midi, \n",
    "                'onsets': onsets, 'offsets': offsets, 'instrument_id': instrument_id}\n",
    "    \n",
    "    n_buffers = source_batch['audio'].shape[1] // buffer_size_in_samples\n",
    "    \n",
    "    for buffer_idx in range(n_buffers):\n",
    "        yield get_vst_inputs_for_buffer(buffer_idx=buffer_idx, \n",
    "                                       buffer_size_in_samples=buffer_size_in_samples,\n",
    "                                       n_frames_in_buffer=n_frames_in_buffer,\n",
    "                                       source_batch=source_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f78ef6-a26c-446c-9e5c-05e9d93a9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size_in_samples = 64000\n",
    "n_frames_in_buffer = 500\n",
    "frame_size = buffer_size_in_samples // n_frames_in_buffer\n",
    "\n",
    "print(frame_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774477fa-d54f-485c-84c8-df156d9e545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_sound = None\n",
    "\n",
    "for vst_input_buffer in get_vst_inputs_for_batch(source_batch=eval_sample_batch,\n",
    "                                 buffer_size_in_samples=buffer_size_in_samples,\n",
    "                                 n_frames_in_buffer=n_frames_in_buffer):\n",
    "    \n",
    "    res = model_vst(**vst_input_buffer, state=tf.random.uniform([512]))\n",
    "    print('finished running the model')\n",
    "        \n",
    "    res['f0_hz']                 = tf.reshape(vst_input_buffer['f0_hz'],    [1, n_frames_in_buffer, 1])\n",
    "    res['amplitudes']            = tf.reshape(res['amplitudes'],            [1, n_frames_in_buffer, 1])\n",
    "    res['harmonic_distribution'] = tf.reshape(res['harmonic_distribution'], [1, n_frames_in_buffer, 60])\n",
    "    res['noise_magnitudes']      = tf.reshape(res['noise_magnitudes'],      [1, n_frames_in_buffer, 65])\n",
    "\n",
    "    my_processor_group_vst = get_process_group(n_frames=n_frames_in_buffer, frame_size=frame_size, sample_rate=16000, use_angular_cumsum=False)\n",
    "    \n",
    "    print(f'res.keys={res.keys()}')\n",
    "    \n",
    "    my_control_params = my_processor_group_vst.get_controls(res, verbose=False)\n",
    "    my_synth_audio = my_processor_group_vst.get_signal(my_control_params)\n",
    "    \n",
    "    if accum_sound is None:\n",
    "        accum_sound = my_synth_audio\n",
    "    else:\n",
    "        accum_sound = tf.concat([accum_sound, my_synth_audio], axis=1)\n",
    "    \n",
    "print(\"finally..\")\n",
    "play(accum_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478c351-650b-4e05-90f7-c45fffc159c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e26711-e63b-4096-b19a-f313198818b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_postfix_to_filename(file_path, postfix):\n",
    "    # Split the path into folder, file base, and file extension\n",
    "    folder, file_name = os.path.split(file_path)\n",
    "    file_base, file_ext = os.path.splitext(file_name)\n",
    "    \n",
    "    # Add the postfix to the file base\n",
    "    new_file_base = file_base + postfix\n",
    "\n",
    "    # Combine the new file base with the file extension\n",
    "    new_file_name = new_file_base + file_ext\n",
    "\n",
    "    # Combine the folder with the new file name\n",
    "    new_file_path = os.path.join(folder, new_file_name)\n",
    "    \n",
    "    return new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c66b8e-691e-48cc-8865-ed3a5f2eac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_tflite(ae_model, path):\n",
    "    #ae_copy = get_synthesis_generator(hp)\n",
    "    #ae_copy._build(get_fake_data_synthesis_generator(hp))\n",
    "    #ae_copy(train_sample_batch)\n",
    "    #ae_copy.set_weights(ae_model.get_weights())\n",
    "    \n",
    "    orig_run_without_synths = ae_model.run_without_synths\n",
    "    orig_run_inside_vst = ae_model.run_inside_vst\n",
    "    orig_run_synth_coder_only = ae_model.run_synth_coder_only\n",
    "\n",
    "    ae_model.run_without_synths = True\n",
    "    ae_model.run_inside_vst = True\n",
    "    ae_model.run_synth_coder_only = False\n",
    "\n",
    "    #ae_copy.reverb_module = None\n",
    "    #ae_copy.processor_group = None\n",
    "    \n",
    "    \n",
    "    model_vst = MIDIExpressionAE_VST_IO_Wrapper(ae_model=model, vst_buffer_size=1024, vst_frame_size=1024)\n",
    "    \n",
    "    # build it\n",
    "    def get_fake_vst_inputs(buffer_size_in_samples, n_frames_in_buffer):\n",
    "        # 16 frames == 1 buffer\n",
    "\n",
    "        audio           = tf.random.uniform([buffer_size_in_samples])\n",
    "        loudness_db     = tf.random.uniform([n_frames_in_buffer])\n",
    "        f0_hz           = tf.random.uniform([n_frames_in_buffer])\n",
    "        midi            = tf.random.uniform([n_frames_in_buffer])\n",
    "        onsets          = tf.random.uniform([n_frames_in_buffer])\n",
    "        offsets         = tf.random.uniform([n_frames_in_buffer])\n",
    "        instrument_id   = tf.random.uniform([1])\n",
    "\n",
    "        return audio, loudness_db, f0_hz, midi, onsets, offsets, instrument_id\n",
    "\n",
    "    vst_inputs = get_fake_vst_inputs(buffer_size_in_samples=1024,\n",
    "                                    n_frames_in_buffer=1)\n",
    "\n",
    "    model_vst(*vst_inputs, state=tf.random.uniform([512]))\n",
    "    print('model_vst is built')\n",
    "    \n",
    "    # CONVERT\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n",
    "\n",
    "    # Define the input signature\n",
    "    \n",
    "    # frame size is 64 here\n",
    "    input_signature_offline = (\n",
    "        tf.TensorSpec(shape=[64000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1000], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[512], dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    input_signature_realtime = (\n",
    "        tf.TensorSpec(shape=[1024], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[1], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[512], dtype=tf.float32),\n",
    "    )\n",
    "\n",
    "    # Wrap the model_vst's call method with the call_wrapper function\n",
    "    \n",
    "    def convert_and_save(target_model, save_path):\n",
    "        call_fn = tf.function(model_vst.call, input_signature=input_signature_realtime)\n",
    "        concrete_func = call_fn.get_concrete_function(*input_signature_realtime)\n",
    "\n",
    "        # Convert the model_vst to TFLite\n",
    "        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
    "\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS ]\n",
    "\n",
    "        print('starting conversion..')\n",
    "        tflite_model = converter.convert()\n",
    "        print('conversion finished')\n",
    "\n",
    "        import pathlib\n",
    "\n",
    "        tflite_model_file = pathlib.Path(save_path)\n",
    "        tflite_model_file.write_bytes(tflite_model)\n",
    "        print(f'tflite model saved to {save_path}')\n",
    "    \n",
    "    model_vst.ae.run_synth_coder_only=False\n",
    "    convert_and_save(model_vst, save_path=append_postfix_to_filename(path, '_midi'))\n",
    "    \n",
    "    model_vst.ae.run_synth_coder_only=True\n",
    "    convert_and_save(model_vst, save_path=append_postfix_to_filename(path, '_synth'))\n",
    "    \n",
    "    ae_model.run_without_synths = orig_run_without_synths\n",
    "    ae_model.run_inside_vst = orig_run_inside_vst\n",
    "    ae_model.run_synth_coder_only = orig_run_synth_coder_only\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49a603-02ac-4cbc-bf8e-2f9a45474eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def picke_train_history(epoch, step):\n",
    "    import pickle\n",
    "\n",
    "    train_history_path=f'{log_dir}/losses_history_e{epoch}_s{step}'\n",
    "    eval_train_history_path=f'{log_dir}/eval_losses_history_e{epoch}_s{step}'\n",
    "\n",
    "    with open(train_history_path, \"wb\") as fp:\n",
    "        pickle.dump(losses_history, fp)\n",
    "\n",
    "    with open(eval_train_history_path, \"wb\") as fp:\n",
    "        pickle.dump(eval_losses_history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34f808-1433-4c7c-9e8b-66a9bd2e6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the original Magenta's train_synthesis_generator.py'\n",
    "\n",
    "def train(training_data, training_data_length, training_epochs, start_epoch=1):\n",
    "    \n",
    "    \"\"\"Training loop including evaluation.\"\"\"\n",
    "    start_time = time.time()\n",
    "    loss_helper.reset_metrics()\n",
    "\n",
    "    steps_in_epoch = int(training_data_length) # do not need to divide by hp.batch_size because each item is a batch itself\n",
    "    logging.info(f\"Starting training: epochs={training_epochs}, steps_in_epoch={steps_in_epoch}, start_epoch={start_epoch}\")\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(start_epoch, training_epochs + start_epoch + 1):\n",
    "        for i, data in enumerate(training_data):\n",
    "            step = ((epoch-1) * steps_in_epoch + i) + 1\n",
    "            print(f\"training on epoch={epoch}, step={step}\")\n",
    "\n",
    "            # Run the model and get the loss.\n",
    "            with tf.GradientTape() as tape, tf.GradientTape() as disc_tape:\n",
    "                outputs = model(data, training=True, run_synth_coder_only=hp.run_synth_coder_only)\n",
    "\n",
    "                loss_dict_recon = loss_helper.compute_loss(data, outputs,\n",
    "                                                           synth_coder_only=hp.run_synth_coder_only,\n",
    "                                                           add_synth_loss=hp.add_synth_loss)\n",
    "\n",
    "                if not hp.run_synth_coder_only and hp.use_gan:\n",
    "                    cond, real_outputs, fake_outputs = gan_loss_helper.get_disc_input(outputs)\n",
    "                    D_fake = net_D([cond, fake_outputs])\n",
    "                    D_real = net_D([cond, real_outputs])\n",
    "                    loss_dict_disc = gan_loss_helper.compute_disc_loss(D_fake, D_real)\n",
    "                    loss, loss_dict_gen = gan_loss_helper.compute_gen_loss(D_fake, D_real, loss_dict_recon['total_loss'])\n",
    "                else:\n",
    "                    loss = loss_dict_recon['total_loss']\n",
    "\n",
    "            # Clip and apply gradients.\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, hp.clip_grad)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            loss_helper.update_metrics(loss_dict_recon)\n",
    "            loss_helper.write_summary(loss_dict_recon, writer, 'Train', step)\n",
    "            \n",
    "            losses_history.append(loss_dict_recon)\n",
    "            plotlosses.update({'train_loss': loss_dict_recon['total_loss']})\n",
    "            \n",
    "            # Train discriminator and update GAN loss.\n",
    "            if not hp.run_synth_coder_only and hp.use_gan:\n",
    "                gradients_of_discriminator = disc_tape.gradient(loss_dict_disc['disc_loss'], net_D.trainable_variables)\n",
    "                optimizer_disc.apply_gradients(zip(gradients_of_discriminator, net_D.trainable_variables))\n",
    "                gan_loss_helper.update_metrics(loss_dict_disc)\n",
    "                gan_loss_helper.write_summary(loss_dict_disc, writer, 'Train', step)\n",
    "                gan_loss_helper.update_metrics(loss_dict_gen)\n",
    "                gan_loss_helper.write_summary(loss_dict_gen, writer, 'Train', step)\n",
    "            \n",
    "            plotlosses.send()\n",
    "            \n",
    "        # Print logging summary.\n",
    "        if epoch % hp.log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            current_lr = optimizer._decayed_lr('float32').numpy()\n",
    "            msg = f'| {epoch:6d} epoch | lr {current_lr:02.2e} ' \\\n",
    "                  f'| ms/batch {(elapsed * 1000 / hp.log_interval):5.2f} '\n",
    "            msg = msg + loss_helper.get_loss_log()\n",
    "            loss_helper.reset_metrics()\n",
    "            if not hp.run_synth_coder_only:\n",
    "                msg = msg + gan_loss_helper.get_loss_log()\n",
    "                gan_loss_helper.reset_metrics()\n",
    "            logging.info(msg)\n",
    "\n",
    "            # Print a message on the same line\n",
    "            IPython.display.clear_output()\n",
    "            print(msg)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Evaluate.\n",
    "        if epoch % hp.eval_interval == 0:\n",
    "            eval_start_time = time.time()\n",
    "            \n",
    "            eval_loss_dict = evaluate(evaluation_data, epoch=epoch, step=step)\n",
    "\n",
    "            plotlosses.update({'eval_loss_total': eval_loss_dict['total_loss'],\n",
    "                              'eval_loss_midi': eval_loss_dict['loss_spectral_midi']})\n",
    "            eval_losses_history.append(eval_loss_dict)\n",
    "            \n",
    "            # Synthesize training data.\n",
    "            outputs = model(train_sample_batch, training=True, run_synth_coder_only=hp.run_synth_coder_only)\n",
    "            \n",
    "            save_results(outputs['synth_audio'], train_sample_batch['audio'], log_dir, f'train_{epoch}_synth', hp.sample_rate)\n",
    "            if 'midi_audio' in outputs.keys():\n",
    "                save_results(outputs['midi_audio'], train_sample_batch['audio'], log_dir, f'train_{epoch}_midi', hp.sample_rate)\n",
    "            if hp.write_tfrecord_audio:\n",
    "                write_tensorboard_audio(writer, train_sample_batch, outputs, epoch, tag='Train')\n",
    "\n",
    "            # Synthesize evaluation data.\n",
    "            outputs = model(eval_sample_batch, training=False, run_synth_coder_only=hp.run_synth_coder_only)\n",
    "            save_results(outputs['synth_audio'], eval_sample_batch['audio'], log_dir, f'eval_{epoch}_synth', hp.sample_rate)\n",
    "            if 'midi_audio' in outputs.keys():\n",
    "                save_results(outputs['midi_audio'], eval_sample_batch['audio'], log_dir, f'eval_{epoch}_midi', hp.sample_rate)\n",
    "            if hp.write_tfrecord_audio:\n",
    "                write_tensorboard_audio(writer, eval_sample_batch, outputs, epoch, tag='Eval')\n",
    "            \n",
    "            eval_time_elapsed = time.time() - eval_start_time\n",
    "            logging.info(f\"Evaluation took {eval_time_elapsed*1000}ms\")\n",
    "        \n",
    "        # DDSP Inference training finished.\n",
    "        # Start training Synthesis Generator and dump dataset for expression generator.\n",
    "        if (epoch - start_epoch + 1) >= hp.synth_coder_training_epochs:\n",
    "            hp.run_synth_coder_only = False\n",
    "            if not hp.add_synth_loss:\n",
    "                model.freeze_synth_coder()\n",
    "\n",
    "        # Save weights for the whole model.\n",
    "        if epoch % hp.checkpoint_save_interval == 0:\n",
    "            model.save_weights(f'{log_dir}/e{epoch}_s{step}')\n",
    "            try:\n",
    "                export_to_tflite(ae_model=model, path=f'{log_dir}/e{epoch}_s{step}.tflite')\n",
    "            except Exception as e:\n",
    "                logging.error(traceback.format_exc())\n",
    "                print(\"An exception occurred while exporting to tflite\")\n",
    "            \n",
    "        picke_train_history(epoch=epoch, step=step)\n",
    "        plotlosses.send()\n",
    "\n",
    "\n",
    "def evaluate(evaluation_data, epoch, step):\n",
    "    \"\"\"Evaluating the test set.\"\"\"\n",
    "    eval_loss_helper = ReconLossHelper(hp, eval_recon_loss=True)\n",
    "    start_time = time.time()\n",
    "    for data in evaluation_data:\n",
    "        outputs = model(data, training=False, run_synth_coder_only=hp.run_synth_coder_only)\n",
    "\n",
    "        loss_dict = eval_loss_helper.compute_loss(data, outputs,synth_coder_only=hp.run_synth_coder_only)\n",
    "        eval_loss_helper.update_metrics(loss_dict)\n",
    "\n",
    "    eval_loss_helper.write_mean_summary(writer, 'Eval', step)\n",
    "    msg = f'eval: | epoch {epoch:6d} | eval time: {(time.time() - start_time):3.3f}'\n",
    "    msg = msg + eval_loss_helper.get_loss_log()\n",
    "    logging.info(msg)\n",
    "    \n",
    "    return eval_loss_helper.get_loss_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55c25c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.data_dir = '../data/'\n",
    "hp.multi_instrument = True\n",
    "hp.instrument = 'vn'\n",
    "\n",
    "#hp.data_dir = '/scratch/ssd004/scratch/burakovr/midi_ddsp_urmp_dataset/all_instruments'\n",
    "#hp.multi_instrument = True\n",
    "#hp.instrument = 'all'\n",
    "\n",
    "hp.vst_inference_mode = False\n",
    "hp.train_synth_coder_first = True\n",
    "hp.training_epochs =96 # 5k steps\n",
    "hp.log_interval = 1\n",
    "hp.checkpoint_save_interval = 1\n",
    "hp.eval_interval = 1\n",
    "hp.synth_coder_training_epochs = 32\n",
    "#hp.synth_coder_training_epochs = 15\n",
    "\n",
    "hp.batch_size=16\n",
    "#hp.reverb_length = 16000\n",
    "hp.reverb_length = 48000\n",
    "\n",
    "#experiment_name = \"full_all_train_and_export\"\n",
    "experiment_name = \"full_vn_train_and_export_batchsize_16_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429bde7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From original Magenta's train_synthesis_generator.py'\n",
    "\n",
    "# Load model, create log directory and log file.\n",
    "log_dir = f\"logs/logs_synthesis_generator/{experiment_name}\"\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9e630-241b-48ce-946d-620029a3065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From original Magenta's train_synthesis_generator.py'\n",
    "\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a7cb1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From original Magenta's train_synthesis_generator.py'\n",
    "\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "log_path = os.path.join(log_dir, 'train2.log')\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                  format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "                  handlers=[\n",
    "                    logging.FileHandler(log_path),\n",
    "                    logging.StreamHandler(sys.stdout)]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cddb8c-1c0a-4dc6-8f24-2b77f8f6f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_epoch_path = tf.train.latest_checkpoint(f'{log_dir}/')\n",
    "latest_epoch_weights_name = os.path.basename(latest_epoch_path) if latest_epoch_path else None\n",
    "\n",
    "hp.restore_path=latest_epoch_path\n",
    "print(f'hp.restore_path={hp.restore_path}')\n",
    "print(f'latest_epoch_weights_name={latest_epoch_weights_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04fc3d-85ba-4e55-8a2c-5f0a198e3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "training_data, length_training_data, evaluation_data, length_evaluation_data = get_dataset(hp, training_data_repeats=1)\n",
    "\n",
    "# Filter the data so that we don't use values we don't need\n",
    "\n",
    "# do not keep 'f0_confidence', 'note_active_frame_indices', 'note_active_velocities', 'note_offsets', 'note_onsets', 'power_db',  'recording_id',\n",
    "keys_to_keep = ['audio',\n",
    "                'f0_hz',\n",
    "                'instrument_id',\n",
    "                'loudness_db',\n",
    "                'midi',\n",
    "                'onsets',\n",
    "                'offsets']\n",
    "# TODO: What to do with 'mel'? Is it used?\n",
    "\n",
    "def filter_keys_in_dataset(example):\n",
    "    return {key: example[key] for key in keys_to_keep}\n",
    "\n",
    "training_data = training_data.map(filter_keys_in_dataset)\n",
    "evaluation_data = evaluation_data.map(filter_keys_in_dataset)\n",
    "\n",
    "# For optional debugging purposes\n",
    "training_data = training_data#.take(10)\n",
    "evaluation_data = evaluation_data#.take(10)\n",
    "\n",
    "eval_sample_batch = next(iter(evaluation_data))\n",
    "train_sample_batch = next(iter(training_data))\n",
    "logging.info('Data loaded! Data size: %s', str(length_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0b8b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from midi_ddsp.modules.model.model_vst import MIDIExpressionAE_VST_IO_Wrapper\n",
    "\n",
    "#hp.use_mel = False\n",
    "hp.use_mel = True\n",
    "\n",
    "# Create Synthesis Generator\n",
    "model = get_synthesis_generator(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627ac77",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.info(f\"model.run_synth_coder_only={model.run_synth_coder_only},\\n\"\n",
    "      f\"model.run_without_synths={model.run_without_synths},\\n\"\n",
    "      f\"model.run_inside_vst={model.run_inside_vst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6b899",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model._build(get_fake_data_synthesis_generator(hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5501f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.keras.Model.summary(model, expand_nested=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c130a-8114-4450-a7d6-69cf62b85068",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hp.restore_path:\n",
    "    print(f'restoring from {hp.restore_path}')\n",
    "    model.load_weights(hp.restore_path)\n",
    "    log_dir = os.path.dirname(hp.restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02c21a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oo = model(train_sample_batch)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9237d1f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# From original Magenta's train_synthesis_generator.py'\n",
    "\n",
    "# Create optimizer, loss helper and discriminator.\n",
    "scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=hp.lr, decay_steps=1000, decay_rate=0.99)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=scheduler)\n",
    "\n",
    "loss_helper = ReconLossHelper(hp)\n",
    "gan_loss_helper = GANLossHelper(lambda_recon=hp.lambda_recon, lambda_G=hp.lambda_G, sg_z=hp.sg_z)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=hp.lr_disc)\n",
    "\n",
    "net_D = Discriminator(nhid=hp.discriminator_dim)\n",
    "# 64=instrument_emb_dim\n",
    "z_dim = hp.discriminator_dim + int(hp.multi_instrument) * 64\n",
    "# synth_params_dim = dim(nharmonic + nnoise + amplitude + f0)\n",
    "synth_params_dim = hp.nhramonic + hp.nnoise + 2\n",
    "_ = net_D((tf.random.normal([4, 1000, z_dim]), tf.random.normal([4, 1000, synth_params_dim])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb5fd1-975e-450a-8511-28239ba6e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a0810",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from midi_ddsp.utils.inference_utils import get_process_group\n",
    "\n",
    "my_processor_group = get_process_group(model.n_frames, model.frame_size, model.sample_rate, use_angular_cumsum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595997aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160b190-de2c-40f6-b2d3-2641e11e5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc67fd6-aae1-4f2e-8074-97def3560d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary and hyperparameters.\n",
    "model.summary(print_fn=logging.info)\n",
    "logging.info(str(print_hparams(hp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da9bbf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model_vst = MIDIExpressionAE_VST_IO_Wrapper(ae_model=model, vst_buffer_size=1024, vst_frame_size=1024)\n",
    "#model_vst = MIDIExpressionAE_VST_IO_Wrapper(ae_model=model, vst_buffer_size=64000, vst_frame_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13793-e52c-43ce-b26a-986fd8a964d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restore losses so that we could see them\n",
    "\n",
    "import pickle\n",
    "\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "print(latest_epoch_weights_name)\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open(f'{log_dir}/losses_history_{latest_epoch_weights_name}', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    losses_history = pickle.load(file)\n",
    "\n",
    "with open(f'{log_dir}/eval_losses_history_{latest_epoch_weights_name}', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    eval_losses_history = pickle.load(file) \n",
    "\n",
    "    \n",
    "approx_steps_in_epoch = len(losses_history) // len(eval_losses_history)\n",
    "num_epochs = len(eval_losses_history)\n",
    "num_steps = len(losses_history)\n",
    "print(f'num_epochs={num_epochs}')\n",
    "print(f'num_steps={num_steps}')\n",
    "print(f'approx_steps_in_epoch={approx_steps_in_epoch}')\n",
    "\n",
    "for i in range(len(losses_history)):  \n",
    "    #plotlosses.update({'eval_loss': eval_losses_history[i]['total_loss'], \n",
    "    #                   'train_loss': losses_history[i]['total_loss']})\n",
    "    if i != 0 and i % approx_steps_in_epoch == 0:\n",
    "        epoch = i // approx_steps_in_epoch - 1\n",
    "        print(i, epoch)\n",
    "        plotlosses.update({'eval_loss_total': eval_losses_history[epoch]['total_loss'],\n",
    "                          'eval_loss_midi': eval_losses_history[epoch]['loss_spectral_midi']})\n",
    "\n",
    "    plotlosses.update({'train_loss': losses_history[i]['total_loss']})\n",
    "    \n",
    "plotlosses.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c454c98",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start training loop\n",
    "\n",
    "if hp.restore_path:\n",
    "    bname = os.path.basename(hp.restore_path)\n",
    "    parts = bname.split(sep='_')\n",
    "    start_epoch = int(parts[0][1:])\n",
    "    \n",
    "    if hp.train_synth_coder_first is True and start_epoch > hp.synth_coder_training_epochs:\n",
    "        hp.train_synth_coder_first = False\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "#start_epoch = 16\n",
    "#hp.train_synth_coder_first = False\n",
    "    \n",
    "print(start_epoch)\n",
    "print(hp.train_synth_coder_first)\n",
    "\n",
    "if hp.mode == 'train':\n",
    "    if hp.train_synth_coder_first:\n",
    "      hp.run_synth_coder_only = True\n",
    "      model.train_synth_coder_only()\n",
    "    else:\n",
    "      hp.run_synth_coder_only = False\n",
    "      model.freeze_synth_coder()\n",
    "\n",
    "    train(training_data=training_data, training_data_length=length_training_data, training_epochs=hp.training_epochs, start_epoch=start_epoch)\n",
    "\n",
    "elif hp.mode == 'eval':\n",
    "    hp.run_synth_coder_only = False\n",
    "    evaluate(evaluation_data, epoch=start_epoch, step=int(start_epoch/length_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4a30a-522b-4bc1-bc1d-767c9956cb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23838677",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_losses = [item['total_loss'] for item in losses_history]\n",
    "\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.locator_params(nbins=30)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 20])\n",
    "\n",
    "plt.plot(total_losses, 'g', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "ax\n",
    "\n",
    "#plt.savefig(f'{log_dir}/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d06aec",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_losses = [item['total_loss'] for item in eval_losses_history]\n",
    "#total_losses = [1, 2, 5]\n",
    "\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.locator_params(nbins=30)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 20])\n",
    "\n",
    "plt.plot(total_losses, 'g', label='Validation loss')\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'{log_dir}/val_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b892ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_history_path=f'{log_dir}/losses_history'\n",
    "eval_train_history_path=f'{log_dir}/eval_losses_history'\n",
    "\n",
    "with open(train_history_path, \"wb\") as fp:\n",
    "    pickle.dump(losses_history, fp)\n",
    "\n",
    "with open(eval_train_history_path, \"wb\") as fp:\n",
    "    pickle.dump(eval_losses_history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b56c9-ef76-4184-9b79-c4ac80d4b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audios = next(iter(training_data))\n",
    "\n",
    "outputs = model(input_audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80707f-24d6-4394-b028-09fe7af70818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c25b0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(input_audios['audio'].shape[0]):\n",
    "    print(\"Original audio: \")\n",
    "    play(input_audios['audio'][i])\n",
    "    specplot(input_audios['audio'][i])\n",
    "\n",
    "    print(\"Reconstructed audio: \")\n",
    "    play(outputs['synth_audio'][i])\n",
    "    specplot(outputs['synth_audio'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
