{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Importing our custom ddsp lib and local sparesenet lib\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    midi_ddsp_module_path = os.path.abspath(os.path.join('../../'))\n",
    "    ddsp_module_path = os.path.abspath(os.path.join('../../../ddsp-playground-2/'))\n",
    "    original_midi_ddsp_module_path = 'E:/Code/Projects/TimbreTransfer/original-midi-ddsp/'\n",
    "    original_ddsp_module_path = 'E:/Code/Projects/TimbreTransfer/original-ddsp-for-vst-debugging/'\n",
    "else:\n",
    "    midi_ddsp_module_path = os.path.abspath(os.path.join('../../'))\n",
    "    ddsp_module_path = os.path.abspath(os.path.join('../../../ddsp/ddsp-playground-2/'))\n",
    "    original_midi_ddsp_module_path = None\n",
    "    original_ddsp_module_path = None\n",
    "\n",
    "def apply_module_path(module_path):\n",
    "    print(f\"module_path={module_path}\")\n",
    "    if module_path not in sys.path:\n",
    "      sys.path.append(module_path)\n",
    "      print(f\"appending {module_path} to sys.path\")\n",
    "    else:\n",
    "      print(f\"do not appending {module_path} to sys.path\")\n",
    "\n",
    "apply_module_path(midi_ddsp_module_path)\n",
    "apply_module_path(ddsp_module_path)\n",
    "apply_module_path(original_midi_ddsp_module_path)\n",
    "apply_module_path(original_ddsp_module_path)\n",
    "\n",
    "import sys\n",
    "if platform.system() != 'Windows':\n",
    "    sparsenet_module_path_abs = '/ssd003/home/burakovr/projects/vova/envs/main/lib/python3.8/site-packages/'\n",
    "    apply_module_path(sparsenet_module_path_abs)\n",
    "\n",
    "import midi_ddsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Copyright 2022 The MIDI-DDSP Authors.\n",
    "#  #\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#  #\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#  #\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\n",
    "\"\"\"Training code for Synthesis Generator.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import IPython\n",
    "\n",
    "from keras.utils.layer_utils import print_summary\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import original_midi_ddsp\n",
    "\n",
    "from midi_ddsp.data_handling.get_dataset import get_dataset\n",
    "from midi_ddsp.utils.training_utils import print_hparams, set_seed, \\\n",
    "    save_results, str2bool\n",
    "from midi_ddsp.utils.summary_utils import write_tensorboard_audio\n",
    "#                          from midi_ddsp.hparams_synthesis_generator import hparams as hp\n",
    "from midi_ddsp.hparams_synthesis_generator import hparams_debug as hp\n",
    "from midi_ddsp.modules.recon_loss import ReconLossHelper\n",
    "from midi_ddsp.modules.gan_loss import GANLossHelper\n",
    "from midi_ddsp.modules.get_synthesis_generator import get_synthesis_generator, \\\n",
    "    get_fake_data_synthesis_generator\n",
    "from midi_ddsp.modules.discriminator import Discriminator\n",
    "\n",
    "from ddsp.colab.notebook_utils import play, specplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.data_dir = '../../data/'\n",
    "hp.batch_size = 1 # for saving to TFRecord purposes\n",
    "\n",
    "hp.multi_instrument = False\n",
    "hp.instrument = 'vn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load vn dataset.\n",
    "vn_training_data, vn_length_training_data, vn_evaluation_data, vn_length_evaluation_data = get_dataset(hp, training_data_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vn_eval_sample_batch = next(iter(vn_evaluation_data))\n",
    "vn_train_sample_batch = next(iter(vn_training_data))\n",
    "logging.info('Violin data loaded! Data size: %s', str(vn_length_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vn_training_example = next(iter(vn_training_data))\n",
    "play(vn_training_example['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(vn_length_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cl dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.data_dir = '../../data_cl/'\n",
    "hp.instrument = 'cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load vn dataset.\n",
    "cl_training_data, cl_length_training_data, cl_evaluation_data, cl_length_evaluation_data = get_dataset(hp, training_data_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cl_eval_sample_batch = next(iter(cl_evaluation_data))\n",
    "cl_train_sample_batch = next(iter(cl_training_data))\n",
    "logging.info('Clarinet data loaded! Data size: %s', str(cl_length_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cl_training_example = next(iter(cl_training_data))\n",
    "play(cl_training_example['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### All dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.data_dir = '../../data_all/'\n",
    "hp.instrument = 'all'\n",
    "hp.multi_instrument = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load vn dataset.\n",
    "all_training_data, all_length_training_data, all_evaluation_data, all_length_evaluation_data = get_dataset(hp, training_data_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_eval_sample_batch = next(iter(all_evaluation_data))\n",
    "all_train_sample_batch = next(iter(all_training_data))\n",
    "logging.info('All (multi-instrument) data loaded! Data size: %s', str(all_length_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_training_example = next(iter(all_training_data))\n",
    "play(all_training_example['audio'])\n",
    "print(all_training_example['instrument_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_length_evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create selected eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_selected_eval_dataset(ds, instrument_abb):\n",
    "\n",
    "    all_eval_example = ds.take(40)\n",
    "    batch_size = 1\n",
    "\n",
    "    all_eval_examples_first_20 = []\n",
    "    for i, ex in enumerate(all_eval_example):\n",
    "        for j in range(batch_size):\n",
    "            \n",
    "            if instrument_abb == original_midi_ddsp.data_handling.instrument_name_utils.INST_ID_TO_ABB_DICT[int(ex['instrument_id'][j])]:\n",
    "                print(f'ex#{i} i#{j} = #{i*batch_size+j}')\n",
    "                play(ex['audio'][j])\n",
    "                print(original_midi_ddsp.data_handling.instrument_name_utils.INST_ID_TO_NAME_DICT[int(ex['instrument_id'][j])])\n",
    "                print()\n",
    "\n",
    "                all_eval_examples_first_20.append({k: v[j] for k, v in ex.items()})\n",
    "    \n",
    "    \n",
    "    print('\\n\\n\\n listing gathered samples:')\n",
    "    for i, ex in enumerate(all_eval_examples_first_20):\n",
    "        print(f'#{i}')\n",
    "        play(ex['audio'])\n",
    "        print()\n",
    "        \n",
    "    return all_eval_examples_first_20\n",
    "    \n",
    "listed_examples = list_selected_eval_dataset(ds=all_training_data, instrument_abb='vn')\n",
    "    \n",
    "#selected_audio_examples.append(listed_examples[49])\n",
    "#print(len(selected_audio_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_audio_examples = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize selected eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(audio, f0_confidence, f0_hz, instrument_id, loudness_db,\n",
    "                      note_active_frame_indices, note_active_velocities, note_offsets,\n",
    "                      note_onsets, power_db, recording_id, midi, onsets, offsets):\n",
    "    print(recording_id)\n",
    "    \n",
    "    features = {\n",
    "        'audio': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(audio).numpy()])),\n",
    "        'f0_confidence': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(f0_confidence).numpy()])),\n",
    "        'f0_hz': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(f0_hz).numpy()])),\n",
    "        'instrument_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(instrument_id).numpy()])),\n",
    "        'loudness_db': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(loudness_db).numpy()])),\n",
    "        'note_active_frame_indices': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(note_active_frame_indices).numpy()])),\n",
    "        'note_active_velocities': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(note_active_velocities).numpy()])),\n",
    "        'note_offsets': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(note_offsets).numpy()])),\n",
    "        'note_onsets': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(note_onsets).numpy()])),\n",
    "        'power_db': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(power_db).numpy()])),\n",
    "        'recording_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(recording_id).numpy()])),\n",
    "        'midi': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(midi).numpy()])),\n",
    "        'onsets': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(onsets).numpy()])),\n",
    "        'offsets': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(offsets).numpy()]))\n",
    "    }\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(sample):\n",
    "    \n",
    "    print({k: v.shape for k, v in sample.items()})\n",
    "    \n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        inp=list(sample.values()),\n",
    "        Tout=tf.string,\n",
    "    )\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(filename):\n",
    "    import pandas as pd\n",
    "    selected_audio_examples_ds = tf.data.Dataset.from_tensor_slices(pd.DataFrame.from_dict(selected_audio_examples).to_dict(orient=\"list\"))\n",
    "\n",
    "    for i, ex in enumerate(selected_audio_examples_ds):\n",
    "        print(i)\n",
    "        play(ex['audio'])\n",
    "        print()\n",
    "        \n",
    "    serialized_selected_audio_examples_ds = selected_audio_examples_ds.map(tf_serialize_example)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for serialized_example in serialized_selected_audio_examples_ds:\n",
    "            writer.write(serialized_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_eval_dataset_filename = \"selected_srcs_for_timbre_transfer.tfrecord\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load selected eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_datatypes = {k: v.dtype for k, v in all_training_example.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "    feature_description = {\n",
    "        'audio': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'f0_confidence': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'f0_hz': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'instrument_id': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'loudness_db': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'note_active_frame_indices': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'note_active_velocities': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'note_offsets': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'note_onsets': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'power_db': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'recording_id': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'midi': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'onsets': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        'offsets': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    for key in parsed_example:\n",
    "        parsed_example[key] = tf.io.parse_tensor(parsed_example[key], out_type=feature_datatypes[key])\n",
    "\n",
    "    # parsed_example[\"f0_hz\"] =            tf.reshape(parsed_example[\"f0_hz\"], ),\n",
    "    # parsed_example[\"loudness_db\"] =      parsed_example[\"loudness_db\"],\n",
    "    # parsed_example[\"synth_amplitudes\"] = parsed_example[\"synth_amplitudes\"],\n",
    "    # parsed_example[\"onsets\"] =           parsed_example[\"onsets\"],\n",
    "    # parsed_example[\"offsets\"] =          parsed_example[\"offsets\"],\n",
    "\n",
    "    return parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_selected_eval_dataset(path):\n",
    "\n",
    "    serialized_selected_audio_examples_ds_restored = tf.data.TFRecordDataset(filenames=[path]).map(parse_example)\n",
    "\n",
    "    \n",
    "    for i, ex in enumerate(serialized_selected_audio_examples_ds_restored):\n",
    "        print(i)\n",
    "        play(ex['audio'])\n",
    "        print()\n",
    "    \n",
    "    return serialized_selected_audio_examples_ds_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_eval_dataset = load_selected_eval_dataset(selected_eval_dataset_filename).batch(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_midi_ddsp_model(hp, path, example_input, use_original_model):\n",
    "    if use_original_model:\n",
    "        midi_ddsp_model = original_midi_ddsp.modules.get_synthesis_generator.get_synthesis_generator(hp)\n",
    "    else:\n",
    "        midi_ddsp_model = get_synthesis_generator(hp)\n",
    "\n",
    "    midi_ddsp_model(example_input)\n",
    "    midi_ddsp_model.summary()\n",
    "\n",
    "    midi_ddsp_model.load_weights(path)\n",
    "    synthcoder_model = midi_ddsp_model.synth_coder\n",
    "\n",
    "    tf.keras.Model.summary(midi_ddsp_model, expand_nested=False)\n",
    "    tf.keras.Model.summary(synthcoder_model, expand_nested=False)\n",
    "\n",
    "    return synthcoder_model, midi_ddsp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "midi_ddsp_models_dir = 'E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magenta all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.multi_instrument=True\n",
    "hp.instrument='vn'\n",
    "\n",
    "hp.reverb_length = 48000\n",
    "\n",
    "magenta_synthcoder_all, magenta_midi_ddsp_all = get_midi_ddsp_model(\n",
    "                                    hp=hp,\n",
    "                                    path=os.path.join(midi_ddsp_models_dir, 'magenta_all_full_model/50000'),\n",
    "                                    example_input=all_train_sample_batch,\n",
    "                                    use_original_model=True)\n",
    "\n",
    "oo = magenta_midi_ddsp_all(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.multi_instrument=True\n",
    "hp.instrument='vn'\n",
    "#hp.batch_size = 1\n",
    "\n",
    "hp.reverb_length = 48000\n",
    "hp.use_mel=True\n",
    "\n",
    "my_synthcoder_vn, my_midi_ddsp_vn = get_midi_ddsp_model(\n",
    "                                    hp=hp,\n",
    "                                    path=os.path.join(midi_ddsp_models_dir, 'my_vn_full_model/e235_s28669'),\n",
    "                                    example_input=all_train_sample_batch,\n",
    "                                    use_original_model=False)\n",
    "\n",
    "oo = my_midi_ddsp_vn(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.multi_instrument=True\n",
    "hp.instrument='all'\n",
    "#hp.batch_size = 1\n",
    "\n",
    "hp.reverb_length = 16000\n",
    "\n",
    "my_synthcoder_all, my_midi_ddsp_all = get_midi_ddsp_model(\n",
    "                                    hp=hp,\n",
    "                                    path=os.path.join(midi_ddsp_models_dir, 'my_all_full_model/e24_s23975'),\n",
    "                                    example_input=all_train_sample_batch,\n",
    "                                    use_original_model=False)\n",
    "\n",
    "oo = my_midi_ddsp_all(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthcoder vn low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.multi_instrument=False\n",
    "hp.instrument='vn'\n",
    "\n",
    "hp.reverb_length = 16000\n",
    "\n",
    "synthcoder_vn_low, midi_ddsp_vn_low = get_midi_ddsp_model(hp=hp, path='logs/logs_synthesis_generator/full_vn_train_and_export/e10_s1939', example_input=vn_eval_sample_batch, use_original_model=False)\n",
    "\n",
    "oo = midi_ddsp_vn_low(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthcoder vn high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hp.multi_instrument=False\n",
    "hp.instrument='vn'\n",
    "\n",
    "hp.reverb_length = 16000\n",
    "\n",
    "#synthcoder_vn_high, midi_ddsp_vn_high = get_midi_ddsp_model(hp=hp, path='logs/logs_synthesis_generator/full_vn_train_and_export/e31_s6013', example_input=vn_eval_sample_batch, use_original_model=False)\n",
    "synthcoder_all_high, midi_ddsp_all_high = magenta_synthcoder_all, magenta_midi_ddsp_all\n",
    "\n",
    "oo = midi_ddsp_all_high(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SynthCoder ablation (without mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.multi_instrument=True\n",
    "hp.instrument='all'\n",
    "\n",
    "hp.reverb_length = 16000\n",
    "hp.use_mel = False\n",
    "\n",
    "synthcoder_no_mel_all, midi_ddsp_no_mel_all = get_midi_ddsp_model(hp=hp, \n",
    "                                                                  path=os.path.join(midi_ddsp_models_dir, 'ablation_study_vn_synthcoder_witout_mel/e300_s249599'), \n",
    "                                                                  example_input=vn_eval_sample_batch, \n",
    "                                                                  use_original_model=False)\n",
    "\n",
    "oo = midi_ddsp_no_mel_all(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SynthCoder ablation (without mel) weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.multi_instrument=True\n",
    "hp.instrument='all'\n",
    "\n",
    "hp.reverb_length = 48000\n",
    "hp.use_mel = False\n",
    "\n",
    "synthcoder_no_mel_all_weird, midi_ddsp_no_mel_all_weird = get_midi_ddsp_model(hp=hp, \n",
    "                                                                  path=os.path.join(midi_ddsp_models_dir, 'weird_synthcoder_without_mel/e16_s9983'), \n",
    "                                                                  example_input=vn_eval_sample_batch, \n",
    "                                                                  use_original_model=False)\n",
    "\n",
    "oo = midi_ddsp_no_mel_all_weird(vn_training_example)\n",
    "play(oo['synth_audio'])\n",
    "play(oo['midi_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(magenta_midi_ddsp_all(all_training_example)['midi_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_audio(synthcoder_output, processor_group):\n",
    "    my_control_params = processor_group.get_controls(synthcoder_output, verbose=False)\n",
    "    my_synth_audio = processor_group.get_signal(my_control_params)\n",
    "    return my_synth_audio\n",
    "\n",
    "def downsample_tensor(tensor, factor):\n",
    "    if len(tensor.shape) == 3:\n",
    "        # tensor is 3D\n",
    "        batch_size, time_length, n = tensor.shape\n",
    "        assert time_length % factor == 0, \"Time axis length must be divisible by the downsample factor.\"\n",
    "\n",
    "        # Reshape the tensor to prepare for downsampling\n",
    "        tensor_reshaped = tf.reshape(tensor, [batch_size, time_length // factor, factor, n])\n",
    "\n",
    "        # Take the mean along the new axis, which was originally the time axis\n",
    "        downsampled_tensor = tf.reduce_mean(tensor_reshaped, axis=2)\n",
    "    elif len(tensor.shape) == 2:\n",
    "        # tensor is 2D\n",
    "        batch_size, time_length = tensor.shape\n",
    "        assert time_length % factor == 0, \"Time axis length must be divisible by the downsample factor.\"\n",
    "\n",
    "        # Reshape the tensor to prepare for downsampling\n",
    "        tensor_reshaped = tf.reshape(tensor, [batch_size, time_length // factor, factor])\n",
    "\n",
    "        # Take the mean along the new axis, which was originally the time axis\n",
    "        downsampled_tensor = tf.reduce_mean(tensor_reshaped, axis=2)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "    return downsampled_tensor\n",
    "\n",
    "def downsample_synth_params(synth_params: dict, factor):\n",
    "    if factor > 1:\n",
    "        return {k: downsample_tensor(v, factor) for k, v in synth_params.items()}\n",
    "    else:\n",
    "        return synth_params\n",
    "\n",
    "def downsample_inputs(inputs: dict, factor):\n",
    "    if factor > 1:\n",
    "        return {k: downsample_tensor(v, factor) if k != 'audio' else v for k, v in inputs.items()}\n",
    "    else:\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_tensor_3d_as_image(tensor, title):\n",
    "    # Ensure the tensor is a NumPy array\n",
    "    tensor_np = tensor.numpy()\n",
    "\n",
    "    # Combine the batch_size and n dimensions into a single dimension\n",
    "    combined_shape = (tensor_np.shape[0] * tensor_np.shape[2], tensor_np.shape[1])\n",
    "    tensor_2d = tensor_np.reshape(combined_shape)\n",
    "\n",
    "    plot_tensor_2d_as_image(tensor=tensor_2d, title=title)\n",
    "\n",
    "def plot_tensor_2d_as_image(tensor, title, savepath=None, saveprefix=None, showfig=True):\n",
    "    # Create a heatmap using Plotly\n",
    "    fig = go.Figure(go.Heatmap(z=tf.transpose(tensor), colorscale='Magma'))\n",
    "\n",
    "    # Customize the axis labels and title\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Samples\",\n",
    "        yaxis_title=\"\",\n",
    "        title=title,\n",
    "        width=32*37,  # Width in pixels (32 inches * 37 pixels per inch)\n",
    "        height=16*37,  # Height in pixels (10 inches * 37 pixels per inch)\n",
    "        autosize=False,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "\n",
    "    if showfig:\n",
    "        # Display the visualization\n",
    "        fig.show()\n",
    "    \n",
    "    if savepath:\n",
    "                \n",
    "        if not os.path.exists(savepath):\n",
    "            # Create the folder if it doesn't exist\n",
    "            os.makedirs(savepath)\n",
    "            \n",
    "        pio.write_image(fig, os.path.join(savepath, f'{saveprefix}_{title}.png'))\n",
    "        \n",
    "def line_plot_tensor_2d(tensor, title, yaxis_title=\"\", max_yaxis=None, savepath=None, saveprefix=None, showfig=True):\n",
    "    # Convert the tensor to a 1D array\n",
    "    fundamental_frequencies = tf.squeeze(tensor)\n",
    "\n",
    "    # Create the time axis\n",
    "    time_axis = np.arange(0, len(fundamental_frequencies))\n",
    "\n",
    "    # Create a line plot using Plotly\n",
    "    fig = go.Figure(go.Scatter(x=time_axis, y=fundamental_frequencies, mode='lines'))\n",
    "\n",
    "    # Customize the axis labels and title\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Frames\",\n",
    "        yaxis_title=yaxis_title,\n",
    "        title=title,\n",
    "        width=32*37,  # Width in pixels (32 inches * 37 pixels per inch)\n",
    "        height=16*37,  # Height in pixels (10 inches * 37 pixels per inch)\n",
    "        autosize=False,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "\n",
    "    # Set the maximum value for the y-axis\n",
    "    if max_yaxis is not None:\n",
    "        fig.update_layout(yaxis=dict(range=[0, max_yaxis]))\n",
    "    \n",
    "    # Display the visualization\n",
    "    if showfig:\n",
    "        # Display the visualization\n",
    "        fig.show()\n",
    "    \n",
    "    if savepath:\n",
    "        \n",
    "        if not os.path.exists(savepath):\n",
    "            # Create the folder if it doesn't exist\n",
    "            os.makedirs(savepath)\n",
    "        \n",
    "        pio.write_image(fig, os.path.join(savepath, f'{saveprefix}_{title}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from copy import deepcopy\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def read_wav(filename, dir='../../data_synths', target_sample_rate = 16000):\n",
    "    audio_file = os.path.join(dir, filename)\n",
    "    audio_data, original_sample_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    resampled_audio_data = librosa.resample(audio_data, original_sample_rate, target_sample_rate)\n",
    "    #resampled_audio_data = audio_data\n",
    "\n",
    "    # Convert the resampled audio data to a TensorFlow tensor\n",
    "    resampled_audio_tensor = tf.convert_to_tensor(resampled_audio_data, dtype=tf.float32)\n",
    "\n",
    "    return resampled_audio_tensor\n",
    "\n",
    "\n",
    "def save_wav(audio_tensor, filename, dir, target_sample_rate=16000):\n",
    "    \n",
    "    midi_ddsp.utils.audio_io.save_wav(wav=audio_tensor, path=os.path.join(dir, filename), sample_rate=target_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi_ddsp.data_handling.instrument_name_utils import INST_ID_TO_NAME_DICT\n",
    "from midi_ddsp.utils.audio_io import tf_log_mel\n",
    "\n",
    "def play_results(inputs, outputs, experiment_name,\n",
    "                 show_midi_ddsp=True,\n",
    "                 plot_inputs=False,\n",
    "                 plot_synthcoder=False,\n",
    "                 plot_midi_ddsp=False,\n",
    "                 plot_io_diff=False,\n",
    "                 plot_expression_features=False,\n",
    "                 fig_savepath=None,\n",
    "                 showfig=True):\n",
    "\n",
    "    for batch_idx in range(outputs['synth_audio'].shape[0]):\n",
    "        \n",
    "        pref=f'{batch_idx}'\n",
    "        \n",
    "        instrument_name = INST_ID_TO_NAME_DICT[int(inputs[\"instrument_id\"][batch_idx])]\n",
    "        \n",
    "        if \"instrument_id_gt\" in inputs is not None:\n",
    "            instrument_name_gt = INST_ID_TO_NAME_DICT[int(inputs[\"instrument_id_gt\"][batch_idx])]\n",
    "        else:\n",
    "            instrument_name_gt = instrument_name\n",
    "        \n",
    "        input_audio = inputs['audio'][batch_idx]\n",
    "        input_mel = None\n",
    "        if plot_inputs:\n",
    "            input_mel = tf_log_mel(audio=input_audio,\n",
    "                 sample_rate=16000,\n",
    "                 win_length=64,\n",
    "                 hop_length=64,\n",
    "                 n_fft=1024,\n",
    "                 num_mels=64,\n",
    "                 fmin=40,\n",
    "                 pad_end=True)\n",
    "            plot_tensor_2d_as_image(input_mel, title='Input audio', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "            line_plot_tensor_2d(inputs['f0_hz'][0], title='f0', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "        print(f'{experiment_name}: instrument={instrument_name_gt}: ground truth: ')\n",
    "        play(input_audio)\n",
    "        \n",
    "        synthcoder_audio = outputs['synth_audio'][batch_idx]\n",
    "        synthcoder_mel = None\n",
    "        if plot_synthcoder:\n",
    "            synthcoder_mel = tf_log_mel(audio=synthcoder_audio,\n",
    "                 sample_rate=16000,\n",
    "                 win_length=64,\n",
    "                 hop_length=64,\n",
    "                 n_fft=1024,\n",
    "                 num_mels=64,\n",
    "                 fmin=40,\n",
    "                 pad_end=True)\n",
    "                \n",
    "            plot_tensor_2d_as_image(synthcoder_mel, title='SynthCoder output audio', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "            plot_tensor_2d_as_image(outputs['synth_params']['noise_magnitudes'][batch_idx], title='SynthCoder output noise magnitudes', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "            plot_tensor_2d_as_image(outputs['synth_params']['harmonic_distribution'][batch_idx], title='SynthCoder output harmonic distribution', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "        if plot_io_diff:\n",
    "            spectrogram_difference = tf.abs(synthcoder_mel - input_mel)\n",
    "            plot_tensor_2d_as_image(spectrogram_difference, title='SynthCoder IO diff', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "        print(f'{experiment_name}: instrument={instrument_name}: synth_audio: ')\n",
    "        play(synthcoder_audio)\n",
    "\n",
    "\n",
    "        if show_midi_ddsp:\n",
    "\n",
    "            if plot_expression_features:\n",
    "                line_plot_tensor_2d(outputs['conditioning_dict']['attack'][batch_idx], title='Attack', max_yaxis=1, savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "                line_plot_tensor_2d(outputs['conditioning_dict']['vibrato'][batch_idx], title='Vibrato', max_yaxis=1, savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "                line_plot_tensor_2d(outputs['conditioning_dict']['volume'][batch_idx], title='Volume', max_yaxis=1, savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "            midi_audio = outputs['midi_audio'][batch_idx]\n",
    "            midi_mel = None\n",
    "            if plot_midi_ddsp:\n",
    "                midi_mel = tf_log_mel(audio=midi_audio,\n",
    "                     sample_rate=16000,\n",
    "                     win_length=64,\n",
    "                     hop_length=64,\n",
    "                     n_fft=1024,\n",
    "                     num_mels=64,\n",
    "                     fmin=40,\n",
    "                     pad_end=True)\n",
    "                plot_tensor_2d_as_image(midi_mel, title='ExpressionDecoder output audio', savepath=fig_savepath, showfig=showfig)\n",
    "                plot_tensor_2d_as_image(outputs['midi_synth_params']['noise_magnitudes'][batch_idx], title='ExpressionDecoder output noise magnitudes', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "                plot_tensor_2d_as_image(outputs['midi_synth_params']['harmonic_distribution'][batch_idx], title='ExpressionDecoder output harmonic distribution', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "            if plot_io_diff:\n",
    "                spectrogram_difference = tf.abs(midi_mel - input_mel)\n",
    "                plot_tensor_2d_as_image(spectrogram_difference, title='ExpressionDecoder IO diff', savepath=fig_savepath, saveprefix=pref, showfig=showfig)\n",
    "\n",
    "\n",
    "            print(f'{experiment_name}: instrument={instrument_name}: midi_audio: ')\n",
    "            play(midi_audio)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perform_timbre_transfer(midi_ddsp_model, inputs, target_instrument_abb='vn'):\n",
    "   \n",
    "    inputs_internal = deepcopy(inputs)\n",
    "    \n",
    "    inputs_internal['instrument_id_gt'] = inputs_internal['instrument_id']\n",
    "    \n",
    "    if target_instrument_abb:\n",
    "        target_instrument_id = tf.constant(midi_ddsp.data_handling.instrument_name_utils.INST_ABB_TO_ID_DICT[target_instrument_abb], shape=[1])\n",
    "        batch_size = inputs['audio'].shape[0]\n",
    "        inputs_internal['instrument_id'] = tf.repeat(target_instrument_id, batch_size)\n",
    "        \n",
    "    outputs = midi_ddsp_model(inputs_internal)\n",
    "    \n",
    "    return outputs, inputs_internal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_io(midi_ddsp_model, \n",
    "            inputs, \n",
    "            outputs,\n",
    "            experiment_name,\n",
    "            target_instrument_abb='vn',\n",
    "            plot=False,\n",
    "            fig_subfolder=None,\n",
    "            showfig=False,\n",
    "            save_io_dir=None,\n",
    "            save_io_midi=True,\n",
    "            save_io_synthcoder=False):\n",
    "    \n",
    "    if fig_subfolder:\n",
    "        fig_savepath = os.path.join(save_io_dir, experiment_name, fig_subfolder)\n",
    "    else:\n",
    "        fig_savepath = None\n",
    "        \n",
    "    play_results(inputs=inputs,\n",
    "                outputs=outputs,\n",
    "                experiment_name=experiment_name,\n",
    "                show_midi_ddsp=True,\n",
    "                plot_inputs=plot,\n",
    "                plot_synthcoder=plot,\n",
    "                plot_midi_ddsp=plot,\n",
    "                plot_io_diff=plot,\n",
    "                plot_expression_features=plot,\n",
    "                fig_savepath=fig_savepath,\n",
    "                showfig=showfig)\n",
    "    \n",
    "    if save_io_dir:\n",
    "        \n",
    "        save_io_full_dir = os.path.join(save_io_dir, experiment_name)\n",
    "        \n",
    "        if not os.path.exists(save_io_full_dir):\n",
    "            # Create the folder if it doesn't exist\n",
    "            os.makedirs(save_io_full_dir)\n",
    "        \n",
    "        for batch_idx in range(inputs['audio'].shape[0]):\n",
    "            \n",
    "            original_instrument_abb = midi_ddsp.data_handling.instrument_name_utils.INST_ID_TO_ABB_DICT[int(inputs[\"instrument_id_gt\"][batch_idx])]\n",
    "            save_wav(audio_tensor=inputs['audio'][batch_idx], filename=f'{batch_idx+1}_{original_instrument_abb}_in.wav', dir=save_io_full_dir)\n",
    "            \n",
    "            if save_io_synthcoder:\n",
    "                save_wav(audio_tensor=outputs['synth_audio'][batch_idx], filename=f'{batch_idx+1}_{target_instrument_abb}_sc_out.wav', dir=save_io_full_dir)\n",
    "            \n",
    "            if save_io_midi:\n",
    "                save_wav(audio_tensor=outputs['midi_audio'][batch_idx], filename=f'{batch_idx+1}_{target_instrument_abb}_out.wav', dir=save_io_full_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_timbre_transfer(midi_ddsp_model, \n",
    "                         inputs, \n",
    "                         experiment_name,\n",
    "                         target_instrument_abb='vn',\n",
    "                         plot=False,\n",
    "                         fig_subfolder=None,\n",
    "                         showfig=False,\n",
    "                         save_io_dir=None,\n",
    "                         save_io_midi=True,\n",
    "                         save_io_synthcoder=False):\n",
    "    \n",
    "    outputs, inputs = perform_timbre_transfer(midi_ddsp_model=midi_ddsp_model, inputs=inputs, target_instrument_abb=target_instrument_abb)\n",
    "    \n",
    "    return eval_timbre_transfer(midi_ddsp_model=midi_ddsp_model, \n",
    "                     inputs=input_dict, \n",
    "                     outputs=outputs,\n",
    "                     experiment_name=experiment_name,\n",
    "                     target_instrument_abb=target_instrument_abb,\n",
    "                     plot=plot,\n",
    "                     fig_subfolder=fig_subfolder,\n",
    "                     showfig=showfig,\n",
    "                     save_io_dir=save_io_dir,\n",
    "                     save_io_midi=save_io_midi,\n",
    "                     save_io_synthcoder=save_io_synthcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_timbre_transfer(midi_ddsp_model=magenta_midi_ddsp_all, \n",
    "                     inputs=next(iter(selected_eval_dataset)), \n",
    "                     experiment_name='my_midi_ddsp_vn',\n",
    "                     target_instrument_abb='vn',\n",
    "                     plot=False,\n",
    "                     fig_subfolder=None,\n",
    "                     showfig=False,\n",
    "                     save_io_dir='E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/auto/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_timbre_transfer(midi_ddsp_model=my_midi_ddsp_vn, \n",
    "                     inputs=next(iter(selected_eval_dataset)), \n",
    "                     experiment_name='my_midi_ddsp_vn_sc_to_vn',\n",
    "                     target_instrument_abb='vn',\n",
    "                     plot=True,\n",
    "                     fig_subfolder='specs',\n",
    "                     showfig=False,\n",
    "                     save_io_dir='E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/auto/',\n",
    "                     save_io_midi=False,\n",
    "                     save_io_synthcoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_timbre_transfer(midi_ddsp_model=midi_ddsp_no_mel_all, \n",
    "                     inputs=next(iter(selected_eval_dataset)), \n",
    "                     experiment_name='midi_ddsp_no_mel_all',\n",
    "                     target_instrument_abb='vn',\n",
    "                     plot=True,\n",
    "                     fig_subfolder='specs',\n",
    "                     showfig=False,\n",
    "                     save_io_dir='E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/auto/',\n",
    "                     save_io_midi=False,\n",
    "                     save_io_synthcoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi_ddsp.utils.inference_utils import get_process_group\n",
    "\n",
    "\n",
    "def test_model_resolution(midi_ddsp_model, \n",
    "                               input_dict, \n",
    "                               downsample_factor, \n",
    "                               should_plot_f0, \n",
    "                               experiment_name,\n",
    "                               target_instrument_abb='vn',\n",
    "                               plot=False,\n",
    "                               fig_subfolder=None,\n",
    "                               showfig=False,\n",
    "                               save_io_dir=None,\n",
    "                               save_io_midi=True,\n",
    "                               save_io_synthcoder=False,\n",
    "                               downsampling_target='inputs',\n",
    "                               run_synth_coder_only=True,):\n",
    "    \n",
    "    params_name = 'synth_params' if run_synth_coder_only else 'midi_synth_params'\n",
    "    original_run_synth_coder_only = midi_ddsp_model.run_synth_coder_only\n",
    "    \n",
    "    midi_ddsp_model.run_synth_coder_only = run_synth_coder_only\n",
    "    if downsampling_target == 'inputs':\n",
    "        downsamped_inputs = downsample_inputs(input_dict, factor=downsample_factor)\n",
    "        shapes = {k: v.shape[1] if len(v.shape) >= 2 else 0 for k, v in downsamped_inputs.items()}\n",
    "        print(f\"Number of frames in input params: {shapes}\")\n",
    "        \n",
    "        outputs = midi_ddsp_model(downsamped_inputs)\n",
    "        output_synth_params = outputs[params_name]\n",
    "        \n",
    "    elif downsampling_target == 'outputs':\n",
    "        outputs = midi_ddsp_model(input_dict)\n",
    "        output_synth_params = downsample_synth_params(outputs[params_name], factor=downsample_factor)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"no such downsampling_target: {downsampling_target}\")\n",
    "        \n",
    "    midi_ddsp_model.run_synth_coder_only = original_run_synth_coder_only\n",
    "        \n",
    "    if should_plot_f0:\n",
    "        line_plot_tensor_2d(input_dict['f0_hz'], title=experiment_name)\n",
    "\n",
    "    shapes = {k: v.shape[1] if len(v.shape) >= 2 else 0 for k, v in output_synth_params.items()}\n",
    "    print(f\"Number of frames in synth params: {shapes}\")\n",
    "\n",
    "    processor_group = get_process_group(n_frames=1000//downsample_factor,\n",
    "                                        frame_size=64*downsample_factor,\n",
    "                                        sample_rate=16000,\n",
    "                                        use_angular_cumsum=False)\n",
    "    \n",
    "    \n",
    "    output_audio = get_audio(synthcoder_output=output_synth_params, processor_group=processor_group)\n",
    "    output_audio = midi_ddsp_model.reverb_module(output_audio, reverb_number=input_dict['instrument_id'], training=False)\n",
    "    \n",
    "    outputs[params_name] = output_synth_params\n",
    "    \n",
    "    print(f'output_audio={output_audio}')\n",
    "    \n",
    "    downsamped_inputs['instrument_id_gt'] = downsamped_inputs['instrument_id']\n",
    "\n",
    "    eval_io(midi_ddsp_model=midi_ddsp_model, \n",
    "                     inputs=downsamped_inputs, \n",
    "                     outputs=outputs,\n",
    "                     experiment_name=experiment_name,\n",
    "                     target_instrument_abb=target_instrument_abb,\n",
    "                     plot=plot,\n",
    "                     fig_subfolder=fig_subfolder,\n",
    "                     showfig=showfig,\n",
    "                     save_io_dir=save_io_dir,\n",
    "                     save_io_midi=save_io_midi,\n",
    "                     save_io_synthcoder=save_io_synthcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from midi_ddsp.utils.inference_utils import get_process_group\n",
    "\n",
    "\n",
    "def test_synthcoder_resolution(midi_ddsp_model, input_dict, downsample_factor, should_plot_f0, title, downsampling_target='inputs', \n",
    "                               run_synth_coder_only=True):\n",
    "    params_name = 'synth_params' if run_synth_coder_only else 'midi_synth_params'\n",
    "    original_run_synth_coder_only = midi_ddsp_model.run_synth_coder_only\n",
    "    midi_ddsp_model.run_synth_coder_only = run_synth_coder_only\n",
    "    if downsampling_target == 'inputs':\n",
    "        downsamped_inputs = downsample_inputs(input_dict, factor=downsample_factor)\n",
    "        shapes = {k: v.shape[1] if len(v.shape) >= 2 else 0 for k, v in downsamped_inputs.items()}\n",
    "        print(f\"Number of frames in input params: {shapes}\")\n",
    "\n",
    "        output_synth_params = midi_ddsp_model(downsamped_inputs)[params_name]\n",
    "    elif downsampling_target == 'outputs':\n",
    "        output_synth_params = downsample_synth_params(midi_ddsp_model(input_dict)[params_name], factor=downsample_factor)\n",
    "    else:\n",
    "        raise ValueError(f\"no such downsampling_target: {downsampling_target}\")\n",
    "    midi_ddsp_model.run_synth_coder_only = original_run_synth_coder_only\n",
    "        \n",
    "    if should_plot_f0:\n",
    "        line_plot_tensor_2d(input_dict['f0_hz'], title=title)\n",
    "\n",
    "    shapes = {k: v.shape[1] if len(v.shape) >= 2 else 0 for k, v in output_synth_params.items()}\n",
    "    print(f\"Number of frames in synth params: {shapes}\")\n",
    "\n",
    "    processor_group = get_process_group(n_frames=1000//downsample_factor,\n",
    "                                        frame_size=64*downsample_factor,\n",
    "                                        sample_rate=16000,\n",
    "                                        use_angular_cumsum=False)\n",
    "    \n",
    "    \n",
    "    output_audio = get_audio(synthcoder_output=output_synth_params, processor_group=processor_group)\n",
    "    output_audio = midi_ddsp_model.reverb_module(output_audio, reverb_number=input_dict['instrument_id'], training=False)\n",
    "    \n",
    "    print('input audio:')\n",
    "    play(input_dict['audio'])\n",
    "    \n",
    "    print('output audio:')\n",
    "    play(output_audio)\n",
    "\n",
    "    output_audio_internal = midi_ddsp_model(downsample_synth_params(input_dict, factor=downsample_factor))['synth_audio']\n",
    "    \n",
    "    print('output audio internal:')\n",
    "    play(output_audio_internal)\n",
    "\n",
    "    \n",
    "    return output_audio\n",
    "\n",
    "\n",
    "def test_synthcoder(midi_ddsp_model, input_dict, should_plot_f0, title):\n",
    "    output = midi_ddsp_model(input_dict)\n",
    "\n",
    "    if should_plot_f0:\n",
    "        line_plot_tensor_2d(input_dict['f0_hz'], title=title)\n",
    "\n",
    "    print('input audio:')\n",
    "    play(input_dict['audio'])\n",
    "    \n",
    "    print('output audio:')\n",
    "    play(output['synth_audio'])\n",
    "\n",
    "    return output['synth_audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from midi_ddsp.data_handling.instrument_name_utils import INST_ID_TO_NAME_DICT, INST_ABB_TO_ID_DICT\n",
    "\n",
    "\n",
    "vn_id_constant = tf.constant(INST_ABB_TO_ID_DICT['vn'], shape=[1])\n",
    "\n",
    "vn_training_example_base = deepcopy(vn_training_example)\n",
    "\n",
    "cl_training_example_base = deepcopy(cl_training_example)\n",
    "\n",
    "cl_training_example_oct_up = deepcopy(cl_training_example_base)\n",
    "cl_training_example_oct_up['f0_hz'] = 2 * cl_training_example_oct_up['f0_hz']\n",
    "cl_training_example_oct_up['instrument_id'] = vn_id_constant\n",
    "\n",
    "cl_training_example_oct_up_pw2 = deepcopy(cl_training_example_oct_up)\n",
    "cl_training_example_oct_up_pw2['audio'] = tf.random.uniform(tf.shape(cl_training_example_oct_up_pw2['audio']))\n",
    "cl_training_example_oct_up_pw2['instrument_id'] = vn_id_constant\n",
    "\n",
    "cl_training_example_replaced_urmp_violin = deepcopy(cl_training_example_base)\n",
    "cl_training_example_replaced_urmp_violin['audio'] = vn_training_example['audio']\n",
    "cl_training_example_replaced_urmp_violin['instrument_id'] = vn_id_constant\n",
    "\n",
    "cl_training_example_oct_up_replaced_urmp_violin = deepcopy(cl_training_example_oct_up)\n",
    "cl_training_example_oct_up_replaced_urmp_violin['audio'] = vn_training_example['audio']\n",
    "cl_training_example_oct_up_replaced_urmp_violin['instrument_id'] = vn_id_constant\n",
    "\n",
    "pluck_arpeggio = read_wav('pluck_arpeggio.wav')\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio = deepcopy(cl_training_example_oct_up)\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio['audio'] = tf.reshape(pluck_arpeggio, [1, 64000])\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio['instrument_id'] = vn_id_constant\n",
    "\n",
    "pluck_arpeggio_bass = read_wav('pluck_arpeggio_bass.wav')\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio_bass = deepcopy(cl_training_example)\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio_bass['audio'] = tf.reshape(pluck_arpeggio_bass, [1, 64000])\n",
    "cl_training_example_oct_up_replaced_audio_pluck_arpeggio_bass['instrument_id'] = vn_id_constant\n",
    "\n",
    "vn_orchestra_downward = read_wav('vn_orchestra_downward.wav')\n",
    "cl_training_example_oct_up_replaced_audio_vn_orchestra_downward = deepcopy(cl_training_example_oct_up)\n",
    "cl_training_example_oct_up_replaced_audio_vn_orchestra_downward['audio'] = tf.reshape(vn_orchestra_downward, [1, 64000])\n",
    "cl_training_example_oct_up_replaced_audio_vn_orchestra_downward['instrument_id'] = vn_id_constant\n",
    "\n",
    "\n",
    "instr_name = INST_ID_TO_NAME_DICT[int(cl_training_example_base['instrument_id'])]\n",
    "\n",
    "samples_to_run = {\n",
    "    f'Inference on urmp violin': vn_training_example_base,\n",
    "    f'Inference on urmp clarinet': cl_training_example_base,\n",
    "    f'Inference on urmp clarinet octave up': cl_training_example_oct_up,\n",
    "    f'Inference on urmp clarinet octave up with audio of noise': cl_training_example_oct_up_pw2,\n",
    "    f'Inference on urmp clarinet with audio of urmp violin': cl_training_example_replaced_urmp_violin,\n",
    "    f'Inference on urmp clarinet octave up with audio of urmp violin': cl_training_example_oct_up_replaced_urmp_violin,\n",
    "    f'Inference on urmp clarinet octave up with audio of pluck arp synth': cl_training_example_oct_up_replaced_audio_pluck_arpeggio,\n",
    "    f'Inference on urmp clarinet with audio of bass pluck arp synth': cl_training_example_oct_up_replaced_audio_pluck_arpeggio_bass,\n",
    "    f'Inference on urmp clarinet octave up with audio of kontakt vn orchestra': cl_training_example_oct_up_replaced_audio_vn_orchestra_downward,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_resolution(midi_ddsp_model=my_midi_ddsp_vn, \n",
    "                     input_dict=next(iter(selected_eval_dataset)), \n",
    "                     experiment_name='my_midi_ddsp_vn_downsample_by_8',\n",
    "                     target_instrument_abb='vn',\n",
    "                     plot=True,\n",
    "                     fig_subfolder='specs',\n",
    "                     showfig=False,\n",
    "                     save_io_dir='E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/auto/',\n",
    "                     save_io_midi=True,\n",
    "                     save_io_synthcoder=False,\n",
    "                     downsample_factor=8,\n",
    "                     should_plot_f0=True,\n",
    "                     downsampling_target='inputs',\n",
    "                     run_synth_coder_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_resolution(midi_ddsp_model=my_midi_ddsp_vn, \n",
    "                     input_dict=next(iter(selected_eval_dataset)), \n",
    "                     experiment_name='my_midi_ddsp_vn_sc_downsample_by_8',\n",
    "                     target_instrument_abb='vn',\n",
    "                     plot=True,\n",
    "                     fig_subfolder='specs',\n",
    "                     showfig=False,\n",
    "                     save_io_dir='E:/Code/TimbreTransfer_ExperimentExamples/MIDI_DDSP/auto/',\n",
    "                     save_io_midi=False,\n",
    "                     save_io_synthcoder=True,\n",
    "                     downsample_factor=8,\n",
    "                     should_plot_f0=True,\n",
    "                     downsampling_target='inputs',\n",
    "                     run_synth_coder_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k, test_sample in samples_to_run.items():\n",
    "    \n",
    "    test_synthcoder_resolution(midi_ddsp_model=my_midi_ddsp_vn,\n",
    "                               input_dict=test_sample,\n",
    "                               downsample_factor=8,\n",
    "                               should_plot_f0=True,\n",
    "                               title=k,\n",
    "                               downsampling_target='inputs',\n",
    "                              run_synth_coder_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_synthcoder_resolution(synthcoder_model=synthcoder,\n",
    "                           input_dict=cl_training_example,\n",
    "                           downsample_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_synthcoder_resolution(synthcoder_model=synthcoder,\n",
    "                           input_dict=cl_training_example,\n",
    "                           downsample_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_synthcoder_resolution(synthcoder_model=synthcoder,\n",
    "                           input_dict=cl_training_example,\n",
    "                           downsample_factor=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_synthcoder_resolution(synthcoder_model=synthcoder,\n",
    "                           input_dict=cl_training_example,\n",
    "                           downsample_factor=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "cl_training_example_with_vn_instrument = deepcopy(cl_training_example)\n",
    "cl_training_example_with_vn_instrument['instrument_id'] = tf.constant([20])\n",
    "\n",
    "print(cl_training_example['instrument_id'])\n",
    "print(cl_training_example['instrument_id'].shape)\n",
    "print(cl_training_example_with_vn_instrument['instrument_id'])\n",
    "print(cl_training_example_with_vn_instrument['instrument_id'].shape)\n",
    "\n",
    "play(midi_ddsp_model(cl_training_example)['synth_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cl_training_example['f0_hz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(midi_ddsp_models_dir, 'my_vn_full_model/losses_history_e235_s28669'), 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    losses_history = pickle.load(file)\n",
    "\n",
    "with open(os.path.join(midi_ddsp_models_dir, 'my_vn_full_model\\\\eval_losses_history_e235_s28669'), 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    eval_losses_history = pickle.load(file) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example data\n",
    "train_loss = [0.5, 0.4, 0.3, 0.2, 0.15, 0.1]\n",
    "eval_loss = [0.6, 0.5, 0.4, 0.3, 0.25, 0.2]\n",
    "\n",
    "# Create a trace for training loss\n",
    "train_trace = go.Scatter(\n",
    "    x=list(range(1, len(train_loss) + 1)),\n",
    "    y=train_loss,\n",
    "    mode='lines',\n",
    "    name='Train Loss'\n",
    ")\n",
    "\n",
    "# Create a trace for evaluation loss\n",
    "eval_trace = go.Scatter(\n",
    "    x=list(range(1, len(eval_loss) + 1)),\n",
    "    y=eval_loss,\n",
    "    mode='lines',\n",
    "    name='Eval Loss'\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Training and Evaluation Loss',\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(title='Loss')\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[train_trace, eval_trace], layout=layout)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
